{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cc885b8",
   "metadata": {},
   "source": [
    "\n",
    "# Lecture 4: OLS Estimator for Multiple Linear Regression (MLR) (Completed version)\n",
    "\n",
    "**Overview**\n",
    "\n",
    "- Warm up: recall OLS in 2D (line through a scatter)\n",
    "\n",
    "- Extend to multiple regression: visualize as a plane in 3D\n",
    "\n",
    "- See how residuals look in 2D and 3D (red segments)\n",
    "\n",
    "- Slice the plane to understand partial effects (hold one variable fixed)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd456df",
   "metadata": {},
   "source": [
    "## üå± Warm-up ‚Äì Connecting to Last Week\n",
    "\n",
    "**Recall:** In simple linear regression (SLR) we had **dots and a line**.  \n",
    "- **What does the OLS line do?** It‚Äôs the **best-fit line through the data**.  \n",
    "\n",
    "<hr style=\"border: 1px dotted #bbb;\">\n",
    "\n",
    "Formally, the **SLR model** is:  \n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x + u\n",
    "$$  \n",
    "\n",
    "- $\\beta_0$: intercept  \n",
    "- $\\beta_1$: slope (effect of $x$ on $y$ in the model)  \n",
    "- $u$: error term (everything else not captured by $x$)  \n",
    "\n",
    "OLS chooses $\\hat\\beta_0, \\hat\\beta_1$ to **minimize the sum of squared errors (SSE)**:  \n",
    "\n",
    "$$\n",
    "\\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n \\big(y_i - (\\beta_0 + \\beta_1 x_i)\\big)^2\n",
    "$$  \n",
    "\n",
    "‚ö†Ô∏è **Caution:** This is a **statistical effect in the model**, not automatically a causal effect ‚Äî causality needs stronger assumptions.  \n",
    "*Example:* üçï Pizza consumption vs. group project grades ‚Üí Teams that meet more often might eat more pizza and do better on projects, but the driver is time spent collaborating. Pizza itself isn‚Äôt boosting grades. \n",
    "\n",
    "<hr style=\"border: 1px dotted #bbb;\">\n",
    "\n",
    "**What happens if we have more regressors?**  \n",
    "For today's class, we will focus on the case of **two regressors**.  Do we still have *dots and a line*?\n",
    "\n",
    "‚ùå Short answer: No!  \n",
    "‚úÖ Instead: we have **dots in 3D space** and OLS fits a **plane** through them.  \n",
    "\n",
    "üëâ Today‚Äôs goal: see that **line ‚Üí plane ‚Üí slice** gives us intuition for *multiple regression*.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95768d90",
   "metadata": {},
   "source": [
    "## üì¶ Required libraries\n",
    "We‚Äôll use a few standard Python libraries in this lab:\n",
    "\n",
    "- [`numpy`](https://numpy.org/) : generate data and do calculations.\n",
    "- [`statsmodels`](https://www.statsmodels.org/) : make 2D and 3D plots.\n",
    "- [`matplotlib`](https://matplotlib.org/) : run OLS regressions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8b04ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's install and import the required libraries together!\n",
    "!pip install numpy statsmodels matplotlib --quiet\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# For 3D plotting\n",
    "from mpl_toolkits.mplot3d import Axes3D  \n",
    "\n",
    "# For animations in Jupyter\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0260ac",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e0dc6",
   "metadata": {},
   "source": [
    "## üéØ Tiny Toy Dataset (5 points) ‚Äî used in all sections\n",
    "\n",
    "We‚Äôll use this small dataset for all the visuals in today‚Äôs lab:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c}\n",
    "\\text{Individual }(i) & \\text{Education }(x_{1i}) & \\text{Experience }(x_{2i}) & \\text{Wage }(y_i) \\\\\\hline\n",
    "1 & 10 & 1 & 20 \\\\\n",
    "2 & 12 & 4 & 24 \\\\\n",
    "3 & 14 & 5 & 27 \\\\\n",
    "4 & 16 & 9 & 31 \\\\\n",
    "5 & 18 & 7 & 34 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "<hr style=\"border: 1px dotted #bbb;\">\n",
    "\n",
    "**Big picture.**\n",
    "- In simple linear regression (SLR) with one regressor (e.g., education), OLS fits a **line** through a scatter of points.  \n",
    "- In multiple linear regression (MLR) with two regressors (education and experience), each observation is a point in **3D space**, and OLS fits a **plane**.  \n",
    "\n",
    "**Partial effect (what a coefficient means).**\n",
    "- The coefficient on a regressor (say, education) is the **slope of the plane in that direction**, *holding the other regressor fixed* (experience).  \n",
    "- A **slice** of the plane at a fixed value of the other variable is a **line** whose slope equals the partial effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f9023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "\n",
    "educ = np.array([10, 12, 14, 16, 18])   # x1\n",
    "exper = np.array([ 1,  4,  5,  9,  7])   # x2\n",
    "wage  = np.array([20, 24, 27, 31, 34])   # y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd9377",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318d209",
   "metadata": {},
   "source": [
    "### 1) Simple OLS in 2D: draw a line through a scatter of points\n",
    "\n",
    "- **Goal.** Find a line  \n",
    "  $$\n",
    "  \\hat{y}_i = \\hat\\beta_0 + \\hat\\beta_1 x_i\n",
    "  $$  \n",
    "  that best summarizes the relation between $y$ and $x$.\n",
    "\n",
    "<hr style=\"border: 1px dotted #bbb;\">\n",
    "\n",
    "- **OLS idea.** Choose $\\hat\\beta_0, \\hat\\beta_1$ to **minimize the mean squared error (MSE):**  \n",
    "\n",
    "  $$\n",
    "  \\min_{\\beta_0,\\beta_1} \\; \\frac{1}{n}\\sum_i \\big(y_i - (\\beta_0 + \\beta_1 x_i)\\big)^2\n",
    "  $$\n",
    "\n",
    "<hr style=\"border: 1px dotted #bbb;\">\n",
    "\n",
    "- **Residual (estimation).** For a point $(x_i,y_i)$, the **residual** is the red vertical segment between the actual point and the fitted line:  \n",
    "\n",
    "  $$\n",
    "  e_i = y_i - \\hat y_i\n",
    "  $$  \n",
    "\n",
    "  OLS makes these red segments **as short as possible on average (squared)**.\n",
    "\n",
    "<hr style=\"border: 1px dotted #bbb;\">\n",
    "\n",
    "- In our toy dataset, let‚Äôs regress **wage on education only**:  \n",
    "\n",
    "  $$\n",
    "  \\text{wage}_i = \\beta_0 + \\beta_1 \\,\\text{educ}_i + u_i\n",
    "  $$  \n",
    "\n",
    "  where:  \n",
    "  - $\\text{wage}_i$: hourly wage (in dollars) for person $i$  \n",
    "  - $\\text{educ}_i$: years of education for person $i$  \n",
    "  - $u_i$: error term (everything else affecting wage not captured by education)  \n",
    "\n",
    "Here‚Äôs the **subset of data** we are using:\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c}\n",
    "\\text{Individual }(i) & \\text{Education }(x_i) & \\text{Wage }(y_i) \\\\\\hline\n",
    "1 & 10 & 20 \\\\\n",
    "2 & 12 & 24 \\\\\n",
    "3 & 14 & 27 \\\\\n",
    "4 & 16 & 31 \\\\\n",
    "5 & 18 & 34 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "> ‚ö†Ô∏è Note: The **full dataset** also has other variables (like experience).  \n",
    "> By running SLR, we are only incorporating **part of the data**.  \n",
    "> Later, in MLR, we‚Äôll see how to bring in those extra columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e900c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLR: wage on education only\n",
    "\n",
    "# Step 1. Raw scatter plot\n",
    "plt.scatter(educ, wage)\n",
    "plt.title(\"Toy dataset: Wage vs Education (raw scatter)\")\n",
    "plt.xlabel(\"Education\"); plt.ylabel(\"Wage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e6ee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2. Fit OLS line\n",
    "\n",
    "# Add constant to X matrix (for intercept)\n",
    "X_slr = sm.add_constant(educ)\n",
    "# Fit the model\n",
    "m_slr = sm.OLS(wage, X_slr).fit()\n",
    "# Get coefficients\n",
    "b0_slr, b1_slr = m_slr.params\n",
    "\n",
    "# Step 3. Add fitted line + residuals\n",
    "\n",
    "# Create line for plotting\n",
    "xs = np.linspace(educ.min()-0.5, educ.max()+0.5, 200) # synthetic x values for line: from educ.min()-0.5 to educ.max()+0.5, 200 points.\n",
    "ys = b0_slr + b1_slr*xs # corresponding y values on line (based on estimated coefficients)\n",
    "\n",
    "plt.scatter(educ, wage, label=\"Data\")\n",
    "plt.plot(xs, ys, linewidth=2, label=\"OLS line\")\n",
    "\n",
    "plt.title(\"SLR: Wage vs Education (with residuals)\")\n",
    "plt.xlabel(\"Education\"); plt.ylabel(\"Wage\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bbaf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Plot residuals (vertical red lines) into the graph we just made\n",
    "\n",
    "# The graph we made above\n",
    "plt.scatter(educ, wage, label=\"Data\")\n",
    "plt.plot(xs, ys, linewidth=2, label=\"OLS line\")\n",
    "\n",
    "# Predicted values and residuals\n",
    "yhat_wage_slr = m_slr.predict(X_slr)\n",
    "resid_slr = wage - yhat_wage_slr\n",
    "\n",
    "mse_slr = np.mean(resid_slr**2)\n",
    "\n",
    "# draw residuals\n",
    "for i in range(len(educ)):\n",
    "    plt.vlines(educ[i], yhat_wage_slr[i], wage[i], color=\"red\", linewidth=1.5)\n",
    "\n",
    "plt.title(\"SLR: Wage vs Education (with residuals)\")\n",
    "plt.xlabel(\"Education\"); plt.ylabel(\"Wage\"); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036de583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect more closely the estimated coefficients and the MSE\n",
    "print(f\"Intercept (Œ≤0): {b0_slr:.2f}\")\n",
    "print(f\"Slope (Œ≤1): {b1_slr:.2f}\")\n",
    "print(f\"MSE: {mse_slr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72437e29",
   "metadata": {},
   "source": [
    "> ### üìä Interpretation of coefficients\n",
    "> - **Estimated OLS line:**  \n",
    ">   $$\n",
    ">   \\hat{y}_i = 2.70 + 1.75 \\, \\text{educ}_i\n",
    ">   $$\n",
    ">\n",
    "> - **Intercept ($\\hat\\beta_0 = 2.70$):** Predicted hourly wage when education = 0.  \n",
    ">   (Not very meaningful in practice, but that‚Äôs the baseline.)  \n",
    ">\n",
    "> - **Slope ($\\hat\\beta_1 = 1.75$):** Each extra year of education is associated with **\\$1.75 higher hourly wage on average**.  \n",
    ">\n",
    "> - **MSE (0.06):** The model‚Äôs average squared error is very small, meaning the line fits these 5 data points almost perfectly.\n",
    ">\n",
    "> **Predicted values and residuals:**\n",
    ">\n",
    "> $$\n",
    "> \\begin{array}{c|c|c|c|c}\n",
    "> \\text{Obs} & \\text{Education }(x_i) & \\text{Actual Wage }(y_i) & \\text{Predicted Wage }(\\hat{y}_i) & \\text{Residual }(e_i) \\\\\\hline\n",
    "> 1 & 10 & 20 & 20.2 & -0.2 \\\\\n",
    "> 2 & 12 & 24 & 23.7 & \\;\\;0.3 \\\\\n",
    "> 3 & 14 & 27 & 27.2 & -0.2 \\\\\n",
    "> 4 & 16 & 31 & 30.7 & \\;\\;0.3 \\\\\n",
    "> 5 & 18 & 34 & 34.2 & -0.2 \\\\\n",
    "> \\end{array}\n",
    "> $$\n",
    ">\n",
    "> These residuals ($e_i$) are exactly the red vertical lines drawn in the plot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47741829",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15021556",
   "metadata": {},
   "source": [
    "### 2) Multiple regression in 3D: fit a plane through points in space\n",
    "\n",
    "- **Setup (intuition).** In simple regression, each individual $i$ is a point $(x_i, y_i)$ in 2D (education, wage).  \n",
    "  With **two regressors**, each individual becomes a point in **3D space**:\n",
    "\n",
    "  $$\n",
    "  (x_{1i}, x_{2i}, y_i) = (\\text{educ}_i,\\ \\text{exper}_i,\\ \\text{wage}_i)\n",
    "  $$\n",
    "\n",
    "  For example:\n",
    "  - Individual 1: $(10,\\ 1,\\ 20)$ ‚Äî one point in 3D.  \n",
    "  - Individual 2: $(12,\\ 4,\\ 24)$ ‚Äî another point.\n",
    "\n",
    "  Plotting all 5 individuals gives a **cloud of dots** in 3D.  \n",
    "  **OLS** chooses the **plane** (instead of a line) that best fits these dots by minimizing the sum of **vertical (in the $y$ direction)** squared distances.\n",
    "\n",
    "<hr style=\"border: 1px dotted #bbb;\">\n",
    "\n",
    "Here is our toy dataset with two regressors (education, experience) and outcome (wage):\n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c}\n",
    "\\text{Individual }(i) & \\text{Education }(x_{1i}) & \\text{Experience }(x_{2i}) & \\text{Wage }(y_i) \\\\\\hline\n",
    "1 & 10 & 1 & 20 \\\\\n",
    "2 & 12 & 4 & 24 \\\\\n",
    "3 & 14 & 5 & 27 \\\\\n",
    "4 & 16 & 9 & 31 \\\\\n",
    "5 & 18 & 7 & 34 \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- Each row is one **individual**.  \n",
    "- Each individual becomes a point in 3D space: $(\\text{educ}_i, \\text{exper}_i, \\text{wage}_i)$.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1926c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intuition: manually enter one point at a time with students\n",
    "fig = plt.figure(figsize=(10,8))  # create a new figure\n",
    "ax = fig.add_subplot(1, 1, 1, projection=\"3d\")  # add a 3D subplot\n",
    "\n",
    "# Individual  1\n",
    "ax.scatter(10, 1, 20, color=\"royalblue\", s=120, edgecolor=\"black\")\n",
    "ax.text(10+0.3, 1+0.3, 20+0.3, \"Individual 1\", color=\"royalblue\")\n",
    "\n",
    "# Individual  2\n",
    "ax.scatter(12, 4, 24, color=\"seagreen\", s=120, edgecolor=\"black\")\n",
    "ax.text(12+0.3, 4+0.3, 24+0.3, \"Individual 2\", color=\"seagreen\")\n",
    "\n",
    "# Individual  3\n",
    "ax.scatter(14, 5, 27, color=\"darkorange\", s=120, edgecolor=\"black\")\n",
    "ax.text(14+0.3, 5+0.3, 27+0.3, \"Individual 3\", color=\"darkorange\")\n",
    "\n",
    "# Individual  4\n",
    "ax.scatter(16, 9, 31, color=\"purple\", s=120, edgecolor=\"black\")\n",
    "ax.text(16+0.3, 9+0.3, 31+0.3, \"Individual 4\", color=\"purple\")\n",
    "\n",
    "# Individual  5\n",
    "ax.scatter(18, 7, 34, color=\"crimson\", s=120, edgecolor=\"black\")\n",
    "ax.text(18+0.3, 7+0.3, 34+0.3, \"Individual 5\", color=\"crimson\")\n",
    "\n",
    "# Labels and style\n",
    "ax.set_xlabel(\"Education\", labelpad=15)\n",
    "ax.set_ylabel(\"Experience\", labelpad=15)\n",
    "ax.text(\n",
    "    x=min(educ)-1, \n",
    "    y=min(exper)-1, \n",
    "    z=max(wage)+1,\n",
    "    s=\"Wage\", fontsize=11, rotation=90, color=\"black\"\n",
    ")\n",
    "\n",
    "ax.view_init(elev=20, azim=120)\n",
    "\n",
    "# Keep axis ranges consistent with padding\n",
    "ax.set_xlim(min(educ)-1, max(educ)+1)\n",
    "ax.set_ylim(min(exper)-1, max(exper)+1)\n",
    "ax.set_zlim(min(wage)-1, max(wage)+1)\n",
    "\n",
    "plt.subplots_adjust(left=0.1, right=0.95, top=0.9, bottom=0.1)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a41dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-rotate the exact 3D graph you built\n",
    "import numpy as np\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=\"3d\")\n",
    "\n",
    "# --- Same points/labels as your static plot ---\n",
    "ax.scatter(10, 1, 20, color=\"royalblue\", s=120, edgecolor=\"black\"); ax.text(10+0.3, 1+0.3, 20+0.3, \"Individual 1\", color=\"royalblue\")\n",
    "ax.scatter(12, 4, 24, color=\"seagreen\", s=120, edgecolor=\"black\");  ax.text(12+0.3, 4+0.3, 24+0.3, \"Individual 2\", color=\"seagreen\")\n",
    "ax.scatter(14, 5, 27, color=\"darkorange\", s=120, edgecolor=\"black\"); ax.text(14+0.3, 5+0.3, 27+0.3, \"Individual 3\", color=\"darkorange\")\n",
    "ax.scatter(16, 9, 31, color=\"purple\", s=120, edgecolor=\"black\");     ax.text(16+0.3, 9+0.3, 31+0.3, \"Individual 4\", color=\"purple\")\n",
    "ax.scatter(18, 7, 34, color=\"crimson\", s=120, edgecolor=\"black\");    ax.text(18+0.3, 7+0.3, 34+0.3, \"Individual 5\", color=\"crimson\")\n",
    "\n",
    "# Axes labels (with manual z-label placement)\n",
    "ax.set_xlabel(\"Education\", labelpad=15)\n",
    "ax.set_ylabel(\"Experience\", labelpad=15)\n",
    "ax.text(x=min(educ)-1, y=min(exper)-1, z=max(wage)+1, s=\"Wage\", fontsize=11, rotation=90, color=\"black\")\n",
    "\n",
    "# Ranges & layout exactly as before\n",
    "ax.set_xlim(min(educ)-1, max(educ)+1)\n",
    "ax.set_ylim(min(exper)-1, max(exper)+1)\n",
    "ax.set_zlim(min(wage)-1, max(wage)+1)\n",
    "ax.view_init(elev=20, azim=120)\n",
    "plt.subplots_adjust(left=0.1, right=0.95, top=0.9, bottom=0.1)\n",
    "\n",
    "# --- Animation: spin around azimuth ---\n",
    "def update(angle):\n",
    "    ax.view_init(elev=20, azim=angle)\n",
    "    return ()\n",
    "\n",
    "anim = FuncAnimation(fig, update, frames=np.linspace(0, 360, 181), interval=50, blit=False)\n",
    "\n",
    "plt.close(fig) \n",
    "HTML(anim.to_jshtml())  # shows the animation inline in Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d49a01f",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px dotted #bbb;\">\n",
    "\n",
    "Now that we‚Äôve placed all 5 individuals in 3D, notice something important:\n",
    "\n",
    "- In **2D (SLR)**, a line summarizes how $y$ changes with $x$.  \n",
    "\n",
    "- In **3D (MLR with two regressors)**, a single line is no longer enough.  \n",
    "\n",
    "    Each point is $(\\text{educ}_i, \\text{exper}_i, \\text{wage}_i)$ in 3D, and OLS fits a **plane** to summarize how wage depends on both regressors.  \n",
    "\n",
    "- Each slope describes change along one axis:  \n",
    "\n",
    "  - $\\hat\\beta_1$: slope in the education direction (holding experience fixed).\n",
    "\n",
    "  - $\\hat\\beta_2$: slope in the experience direction (holding education fixed).  \n",
    "\n",
    "- If we **slice** the plane at a fixed level of one regressor, the slice is a **line** in 2D whose slope equals the partial effect of the other regressor.  \n",
    "\n",
    "<hr style=\"border: 1px dotted #bbb;\">\n",
    "\n",
    "**OLS in 3D:**  \n",
    "- The fitted surface is:  \n",
    "  $$\n",
    "  \\hat y_i = \\hat\\beta_0 + \\hat\\beta_1 \\,\\text{educ}_i + \\hat\\beta_2 \\,\\text{exper}_i\n",
    "  $$\n",
    "- OLS chooses $(\\hat\\beta_0, \\hat\\beta_1, \\hat\\beta_2)$ to minimize the **sum of squared vertical distances** between actual points and the plane:  \n",
    "  $$\n",
    "  \\min_{\\beta_0,\\beta_1,\\beta_2} \\sum_i \\Big(y_i - (\\beta_0 + \\beta_1 \\,\\text{educ}_i + \\beta_2 \\,\\text{exper}_i)\\Big)^2\n",
    "  $$\n",
    "\n",
    "üëâ Same idea as before: in 2D OLS found the best-fitting line, in 3D it finds the best-fitting plane.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d8dc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLR: wage on education and experience (3D plane + residual segments)\n",
    "X_mlr = sm.add_constant(np.column_stack([educ, exper]))  # add constant & combine regressors\n",
    "m_mlr = sm.OLS(wage, X_mlr).fit()\n",
    "b0, b1, b2 = m_mlr.params\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=\"3d\")\n",
    "\n",
    "# Data points\n",
    "ax.scatter(educ, exper, wage, label=\"Data\")\n",
    "\n",
    "# Mesh for the fitted plane\n",
    "E, K = np.meshgrid(\n",
    "    np.linspace(educ.min()-0.5, educ.max()+0.5, 20),\n",
    "    np.linspace(exper.min()-0.5, exper.max()+0.5, 20)\n",
    ")\n",
    "W = b0 + b1*E + b2*K\n",
    "ax.plot_surface(E, K, W, alpha=0.35)\n",
    "\n",
    "# Residual \"vertical\" segments\n",
    "yhat3 = b0 + b1*educ + b2*exper\n",
    "for i in range(len(educ)):\n",
    "    ax.plot([educ[i], educ[i]], [exper[i], exper[i]], [yhat3[i], wage[i]], linewidth=1.5)\n",
    "\n",
    "# Labels, ranges, view\n",
    "ax.set_title(\"MLR: Fit a plane to 5 points (with residual segments)\")\n",
    "ax.set_xlabel(\"Education\", labelpad=12)\n",
    "ax.set_ylabel(\"Experience\", labelpad=12)\n",
    "ax.text(\n",
    "    x=min(educ)-1, \n",
    "    y=min(exper)-1, \n",
    "    z=max(wage)+1,\n",
    "    s=\"Wage\", fontsize=11, rotation=90, color=\"black\"\n",
    ")\n",
    "ax.set_xlim(educ.min()-1, educ.max()+1)\n",
    "ax.set_ylim(exper.min()-1, exper.max()+1)\n",
    "ax.set_zlim(wage.min()-1, wage.max()+1)\n",
    "ax.view_init(elev=20, azim=120)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Pretty print coefficients and fit quality\n",
    "print(f\"Intercept (Œ≤ÃÇ0): {b0:.2f}\")\n",
    "print(f\"Educ slope (Œ≤ÃÇ1): {b1:.2f}\")\n",
    "print(f\"Exper slope (Œ≤ÃÇ2): {b2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed616f1",
   "metadata": {},
   "source": [
    "> ### üìä Interpretation of the MLR fit (5-point dataset)\n",
    "> - **Estimated OLS plane:**  \n",
    ">   $$\n",
    ">   \\hat{y}_i \\;\\approx\\; 3.80 \\;+\\; 1.61\\,\\text{educ}_i \\;+\\; 0.16\\,\\text{exper}_i\n",
    ">   $$\n",
    ">\n",
    "> - **Intercept ($\\hat\\beta_0 = 3.80$):** Predicted wage when education = 0 and experience = 0 (baseline; not always meaningful).\n",
    ">\n",
    "> - **Education slope ($\\hat\\beta_1 = 1.61$):** Holding **experience** fixed, +1 year of education is associated with about **\\$1.61** higher wage on average.\n",
    ">\n",
    "> - **Experience slope ($\\hat\\beta_2 = 0.16$):** Holding **education** fixed, +1 year of experience is associated with about **\\$0.16** higher wage on average.\n",
    ">\n",
    "> - **Residuals:** The red vertical segments in the 3D plot show  \n",
    ">   $$\n",
    ">   e_i = y_i - \\hat{y}_i\n",
    ">   $$\n",
    ">   OLS chooses the plane to make these vertical gaps as small as possible **in the squared sense**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14da589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate the fitted plane + points to view from all angles (with ground projections)\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=\"3d\")\n",
    "\n",
    "# Re-draw the same plot\n",
    "ax.scatter(educ, exper, wage, label=\"Data\")\n",
    "E, K = np.meshgrid(\n",
    "    np.linspace(educ.min()-0.5, educ.max()+0.5, 20),\n",
    "    np.linspace(exper.min()-0.5, exper.max()+0.5, 20)\n",
    ")\n",
    "W = b0 + b1*E + b2*K\n",
    "ax.plot_surface(E, K, W, alpha=0.35, shade=False)  # shade=False to avoid warnings\n",
    "\n",
    "yhat3 = b0 + b1*educ + b2*exper\n",
    "for i in range(len(educ)):\n",
    "    ax.plot([educ[i], educ[i]], [exper[i], exper[i]], [yhat3[i], wage[i]], linewidth=1.5)\n",
    "\n",
    "# --- Prediction at (educ*, exper*) and ground projections ---\n",
    "educ_star  = 15    # <-- change live in class\n",
    "exper_star = 6     # <-- change live in class\n",
    "yhat_star  = b0 + b1*educ_star + b2*exper_star\n",
    "\n",
    "# point on the plane\n",
    "ax.scatter([educ_star], [exper_star], [yhat_star],\n",
    "           s=120, color=\"crimson\", edgecolor=\"black\", label=\"Predicted ≈∑ on plane\")\n",
    "\n",
    "# choose a ground z (bottom of z-limits) for projections\n",
    "z0 = wage.min() - 1\n",
    "\n",
    "# vertical dotted line from ground up to the plane\n",
    "ax.plot([educ_star, educ_star], [exper_star, exper_star],\n",
    "        [z0, yhat_star], linestyle=\":\", linewidth=2, color=\"black\")\n",
    "\n",
    "# ground projection lines along axes (z = z0)\n",
    "xmin, xmax = ax.get_xlim()\n",
    "ymin, ymax = ax.get_ylim()\n",
    "\n",
    "# from experience axis to (educ*, exper*) on ground (move along education at fixed exper=exper*)\n",
    "ax.plot([xmax, educ_star], [exper_star, exper_star],\n",
    "        [z0, z0], linestyle=\":\", linewidth=1.8, color=\"gray\")\n",
    "\n",
    "# from education axis to (educ*, exper*) on ground (move along experience at fixed educ=educ*)\n",
    "ax.plot([educ_star, educ_star], [ymax, exper_star],\n",
    "        [z0, z0], linestyle=\":\", linewidth=1.8, color=\"gray\")\n",
    "\n",
    "# optional: mark the ground base point\n",
    "ax.scatter([educ_star], [exper_star], [z0], s=50, color=\"gray\")\n",
    "\n",
    "# annotate predicted value\n",
    "ax.text(educ_star+0.4, exper_star+0.4, yhat_star+0.4,\n",
    "        f\"≈∑ = {yhat_star:.2f}\", color=\"crimson\")\n",
    "\n",
    "# Labels, ranges, view\n",
    "ax.set_xlabel(\"Education\", labelpad=12)\n",
    "ax.set_ylabel(\"Experience\", labelpad=12)\n",
    "ax.set_zlabel(\"Wage\", labelpad=18)\n",
    "ax.set_xlim(educ.min()-1, educ.max()+1)\n",
    "ax.set_ylim(exper.min()-1, exper.max()+1)\n",
    "ax.set_zlim(wage.min()-1, wage.max()+1)\n",
    "ax.view_init(elev=20, azim=120)\n",
    "\n",
    "def update(angle):\n",
    "    ax.view_init(elev=20, azim=angle)\n",
    "    return ()\n",
    "\n",
    "anim = FuncAnimation(fig, update, frames=np.linspace(0, 360, 181), interval=50, blit=False)\n",
    "\n",
    "plt.close(fig)  # hide static frame\n",
    "HTML(anim.to_jshtml())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362a48c0",
   "metadata": {},
   "source": [
    "<hr style=\"border: 1px dotted #bbb;\">\n",
    "\n",
    "### üîÆ Reading a prediction from the regression plane\n",
    "\n",
    "Suppose we want the predicted wage for an individual with  \n",
    "- **Education = 15** years  \n",
    "- **Experience = 6** years.  \n",
    "\n",
    "OLS gives us a predicted value:  \n",
    "$$\n",
    "\\hat y^* = \\hat\\beta_0 + \\hat\\beta_1 \\cdot 15 + \\hat\\beta_2 \\cdot 6 .\n",
    "$$\n",
    "\n",
    "In the graph:  \n",
    "- The **red point** is $(15,\\;6,\\;\\hat y^*)$ sitting on the regression plane.  \n",
    "- The **black dotted line** shows how we project up from the ground to the plane to find $\\hat y^*$.  \n",
    "- The **gray dotted lines on the ground** connect back to the education and experience axes, so you can *read the input values* directly from the axis ticks.  \n",
    "\n",
    "üëâ This shows how the regression plane lets us compute and visualize **predicted outcomes** for any combination of regressors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea8690",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a50d838",
   "metadata": {},
   "source": [
    "### 3) Slicing the plane ‚áí a line: reading a **partial effect**\n",
    "\n",
    "- **Holding constant.**  \n",
    "  To isolate the effect of **education**, we ‚Äúslice‚Äù the regression plane at a fixed level of **experience** (for example, the median value).  \n",
    "\n",
    "- **What we see.**  \n",
    "  That slice is a **line** in the (education, wage) plane.  \n",
    "  Its slope equals the estimated coefficient on education, $\\hat\\beta_1$.  \n",
    "\n",
    "- **Interpretation.**  \n",
    "  > If education increases by 1 year, while holding experience fixed,  \n",
    "  > wage is predicted to change by **$\\hat\\beta_1$** on average.  \n",
    "\n",
    "üëâ This is what we mean by a **partial effect**: the slope of the plane *in one direction* when the other regressor is kept constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the plane at exper = 6, and rotate to view from all angles\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(1, 1, 1, projection=\"3d\")\n",
    "\n",
    "# Data and fitted plane\n",
    "ax.scatter(educ, exper, wage, label=\"Data\")\n",
    "E, K = np.meshgrid(\n",
    "    np.linspace(educ.min()-0.5, educ.max()+0.5, 20),\n",
    "    np.linspace(exper.min()-0.5, exper.max()+0.5, 20)\n",
    ")\n",
    "W = b0 + b1*E + b2*K\n",
    "ax.plot_surface(E, K, W, alpha=0.35, shade=False)\n",
    "\n",
    "# --- Slice settings ---\n",
    "exper_slice = 6.0  # ‚Üê choose the fixed experience level for the slice (e.g., median or 6)\n",
    "z0, z1 = wage.min() - 1, wage.max() + 1\n",
    "x0, x1 = educ.min() - 1, educ.max() + 1\n",
    "\n",
    "# Draw a translucent vertical panel at exper = exper_slice (to visualize \"holding exper fixed\")\n",
    "Xp, Zp = np.meshgrid(np.linspace(x0, x1, 2), np.linspace(z0, z1, 2))\n",
    "Yp = np.full_like(Xp, exper_slice)\n",
    "ax.plot_surface(Xp, Yp, Zp, alpha=0.10, color=\"gray\", edgecolor=\"none\")  # slice panel\n",
    "\n",
    "# Intersection line: where the OLS plane meets the slice panel\n",
    "x_line = np.linspace(educ.min()-0.5, educ.max()+0.5, 200)\n",
    "y_line = np.full_like(x_line, exper_slice)\n",
    "z_line = b0 + b1*x_line + b2*exper_slice\n",
    "ax.plot(x_line, y_line, z_line, linewidth=3, color=\"crimson\", label=f\"Slice @ exper={exper_slice:g}\")\n",
    "\n",
    "# Optional: label the slice line\n",
    "ax.text(x_line[-1], exper_slice, z_line[-1], \"  slice (partial effect of educ)\", color=\"crimson\")\n",
    "\n",
    "# Residuals (keep or comment out if you want cleaner view)\n",
    "yhat3 = b0 + b1*educ + b2*exper\n",
    "for i in range(len(educ)):\n",
    "    ax.plot([educ[i], educ[i]], [exper[i], exper[i]], [yhat3[i], wage[i]], linewidth=1.2, color=\"black\")\n",
    "\n",
    "# Axes, limits, view\n",
    "ax.set_xlabel(\"Education\", labelpad=12)\n",
    "ax.set_ylabel(\"Experience\", labelpad=12)\n",
    "ax.set_zlabel(\"Wage\", labelpad=18)\n",
    "ax.set_xlim(educ.min()-1, educ.max()+1)\n",
    "ax.set_ylim(exper.min()-1, exper.max()+1)\n",
    "ax.set_zlim(wage.min()-1, wage.max()+1)\n",
    "ax.view_init(elev=20, azim=120)\n",
    "\n",
    "def update(angle):\n",
    "    ax.view_init(elev=20, azim=angle)\n",
    "    return ()\n",
    "\n",
    "anim = FuncAnimation(fig, update, frames=np.linspace(0, 360, 181), interval=50, blit=False)\n",
    "\n",
    "plt.close(fig)  # hide static frame\n",
    "HTML(anim.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a363b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice the plane at a fixed experience level, show the implied line\n",
    "exper_slice = 6.0\n",
    "xs2 = np.linspace(educ.min()-0.5, educ.max()+0.5, 200)\n",
    "w_slice = b0 + b1*xs2 + b2*exper_slice\n",
    "\n",
    "plt.scatter(educ, wage, label=\"Data\", color=\"blue\")\n",
    "plt.plot(xs2, w_slice, color=\"crimson\", linewidth=2,\n",
    "         label=f\"Slice @ exper = {exper_slice:g}\")\n",
    "\n",
    "plt.title(\"Slice of the plane ‚áí line (partial effect of education)\")\n",
    "plt.xlabel(\"Education\")\n",
    "plt.ylabel(\"Wage\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"Partial effect of education (Œ≤ÃÇ1) =\", round(b1, 6))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ac43e1",
   "metadata": {},
   "source": [
    "üí° **Key point:**  \n",
    "- The **slope** with respect to education ($\\hat\\beta_1$) is the **same across all slices**.  \n",
    "- The **intercept** of each slice changes with the fixed value of experience  (shifted by $\\hat\\beta_2 \\times \\text{exper}$).  \n",
    "\n",
    "üëâ So when we change the slice, we move the line up/down, but the **slope stays constant**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2969bbe3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üîé Comparing MLR vs SLR\n",
    "\n",
    "> **SLR (wage ~ educ):**\n",
    ">   $$\n",
    ">   \\hat{y}_i \\;\\approx\\; 2.70 \\;+\\; 1.75 \\,\\text{educ}_i\n",
    ">   $$\n",
    ">   - Slope ($\\hat\\beta_1 = 1.75$): each extra year of education is associated with **\\$1.75 higher wage** on average.  \n",
    ">   - But ‚ö†Ô∏è this ignores **experience**, so part of experience‚Äôs effect is bundled into education‚Äôs slope.\n",
    ">\n",
    "> **MLR (wage ~ educ + exper):**\n",
    ">     $$\n",
    ">     \\hat{y}_i \\;\\approx\\; 3.80 \\;+\\; 1.61 \\,\\text{educ}_i \\;+\\; 0.16 \\,\\text{exper}_i\n",
    ">     $$\n",
    ">     - Education effect drops to **\\$1.61** once we account for experience.  \n",
    ">     - Experience itself has a small positive slope (**\\$0.16** per year).  \n",
    ">\n",
    "> **Takeaway:**  \n",
    "> - In SLR, the slope on education was a bit too high because experience was **omitted** and is positively correlated with education.  \n",
    "> - In MLR, the regression ‚Äúsplits‚Äù the variation correctly: education still matters a lot, but some of the wage differences are explained by experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c060d7d",
   "metadata": {},
   "source": [
    "---\n",
    "## References & Acknowledgments\n",
    "\n",
    "- This teaching material was prepared with the assistance of **OpenAI's ChatGPT (GPT-5)**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2cad9e",
   "metadata": {},
   "source": [
    "**End of lecture notebook.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
